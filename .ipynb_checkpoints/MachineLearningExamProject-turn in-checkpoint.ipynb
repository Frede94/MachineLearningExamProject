{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a1f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import ensemble\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1071419c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load dataset from given filepath.\n",
    "\n",
    "#filepath = \"../datasets/AdultDataSet/adult.data\"\n",
    "filepath = \"C:/Users/Frederik/Documents/Skole/Machine learning/datasets/AdultDataSet/adult.data\"\n",
    "dataSet = pd.read_table(filepath, sep=\",\", header=None);\n",
    "\n",
    "dataSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "015417fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add columns to dataset/ Rename column headers\n",
    "dataSet = dataSet.rename(columns={0: 'age', 1: 'workclass', 2: 'fnlwgt', 3: 'education', 4: 'education-num',\n",
    "                                  5: 'marital-status', 6: 'occupation', 7: 'relationship', 8: 'race', 9: 'sex',\n",
    "                                  10: 'capital-gain', 11: 'capital-loss', 12: 'hours-per-week', 13: 'native-country', 14: 'income'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b7516e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check info of dataset.\n",
    "dataSet.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6051ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle the non-numerical columns in dataset\n",
    "# Creating function, getting the columns, beginning to iterate through them.\n",
    "def handle_non_numerical_data(dataSet):\n",
    "    columns = dataSet.columns.values\n",
    "    for column in columns:\n",
    "        text_digit_vals = {}\n",
    "        # Here, we've added an embedded function that converts,\n",
    "        # the parameter value to whatever the value of that item (as a key) is from the text_digit_vals dictionary.\n",
    "        # We aren't using it just yet, but we're about to.\n",
    "        # Next, while we're iterating through the columns, \n",
    "        # we're going to ask if that column is not either an np.int64 or np.float64.\n",
    "        # If not, then we're going to convert the column to a list of its values, \n",
    "        # then we take the set of that column to get just the unique values.\n",
    "        def convert_to_int(val):\n",
    "            return text_digit_vals[val]\n",
    "\n",
    "        if dataSet[column].dtype != np.int64 and dataSet[column].dtype != np.float64:\n",
    "            column_contents = dataSet[column].values.tolist()\n",
    "            unique_elements = set(column_contents)\n",
    "            x = 0\n",
    "            # Continuing along, for each of the unique elements we find,\n",
    "            # we create a new dictionary key that is that unique element,\n",
    "            # with a value of a new number. Once we've iterated through all of the unique values, \n",
    "            # we then use mapping to map the function we created before to the pandas column.\n",
    "            for unique in unique_elements:\n",
    "                if unique not in text_digit_vals:\n",
    "                    text_digit_vals[unique] = x\n",
    "                    x+=1\n",
    "\n",
    "            dataSet[column] = list(map(convert_to_int, dataSet[column]))\n",
    "\n",
    "    return dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64637590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function to handle non-numerical data\n",
    "# Show changes to the dataset\n",
    "dataSet = handle_non_numerical_data(dataSet)\n",
    "dataSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2f0ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise correlation of columns, excluding NA/null values.\n",
    "corr_matrix = dataSet.corr()\n",
    "corr_matrix[\"income\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Do not touch the target attribute\n",
    "targetAttribute = [\"income\"]\n",
    "\n",
    "# Standardize all the feature attributes\n",
    "scaledAttributes = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\"]\n",
    "\n",
    "fullPipeline = ColumnTransformer([\n",
    "    (\"target\", 'passthrough', targetAttribute),  \n",
    "    (\"scaled\", StandardScaler(), scaledAttributes)\n",
    "    #(\"onehot\", OneHotEncoder(sparse_output=False), originAttribute)\n",
    "])\n",
    "\n",
    "dataPrepared = fullPipeline.fit_transform(dataSet)\n",
    "\n",
    "dataPrepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beaa5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPrepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08e8f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the first 4 columns in dataset.\n",
    "X = dataPrepared[:,0:3]\n",
    "y = dataPrepared[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4761e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ea05d",
   "metadata": {},
   "source": [
    "# Gradient Boosting with Early stopping\n",
    "In order to find the optimal number of trees, i make use of early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfc17aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a GradientBoostingRegressor:\n",
    "gbrt = GradientBoostingRegressor(max_depth=4, n_estimators=400, random_state=45)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "# Measure the validation error at each stage of training to find the optimal number of trees.\n",
    "# The staged_predict() returns an iterator over the predictions made by the ensemble at each iteration of training.\n",
    "\n",
    "errors = [mean_squared_error(y_test, y_pred) for y_pred in gbrt.staged_predict(X_test)]\n",
    "\n",
    "bst_n_estimators = np.argmin(errors) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0ac6e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train another GradientBoostingRegressor using the optimal number of trees:\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60f0d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE on Training set\n",
    "mse = mean_squared_error(y_train, gbrt_best.predict(X_train))\n",
    "print(\"The mean squared error (MSE) on train set: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee3dd91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE on Test set\n",
    "mse = mean_squared_error(y_test, gbrt_best.predict(X_test))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5bd8b2",
   "metadata": {},
   "source": [
    "# Showing Deviance in a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = np.zeros((bst_n_estimators), dtype=np.float64)\n",
    "for i, y_pred in enumerate(gbrt_best.staged_predict(X_test)):\n",
    "    test_score[i] = gbrt_best.loss_(y_test, y_pred)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title(\"Deviance\")\n",
    "plt.plot(\n",
    "    np.arange(bst_n_estimators) + 1,\n",
    "    gbrt_best.train_score_,\n",
    "    \"b-\",\n",
    "    label=\"Training Set Deviance\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(bst_n_estimators) + 1, test_score, \"r-\", label=\"Test Set Deviance\"\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting Iterations\")\n",
    "plt.ylabel(\"Deviance\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
